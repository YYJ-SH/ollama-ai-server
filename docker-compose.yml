version: "3.9"

services:
  ollama_gpu0:
    image: ollama/ollama:0.3.13
    container_name: ollama_gpu0
    ports:
      - "11436:11434"
    volumes:
      - ollama_gpu0_data:/root/.ollama        # 모델 캐시 볼륨
    restart: always
    runtime: nvidia
    environment:
      # ---- GPU 설정 ----
      - NVIDIA_VISIBLE_DEVICES=0
      - NVIDIA_DRIVER_CAPABILITIES=all
      - OLLAMA_LLM_LIBRARY=cuda_v12           # ✅ CUDA 12 고정
      - OLLAMA_FLASH_ATTENTION=true            # ✅ 플래시어텐션 활성화
      - OLLAMA_GPU_LAYERS=20                   # ✅ 3060 12GB에 안정적인 레이어 수
      - OLLAMA_NUM_PARALLEL=1                  # ✅ 동시 요청 제한
      - OLLAMA_MAX_LOADED_MODELS=1             # ✅ VRAM 누수 방지

      # ---- 퍼포먼스 & 안정성 ----
      - OLLAMA_KEEP_ALIVE=30m                  # 모델 유지시간 (필요시 -1로 무제한)
      - OLLAMA_BATCH_SIZE=128                  # ✅ GPU VRAM 안정화를 위해 128로 축소
      - OLLAMA_KV_SIZE=2048                    # ✅ context window (2048~4096 가능)
      - OLLAMA_DEBUG=INFO
      - OLLAMA_TMPDIR=/tmp                     # temp I/O를 빠른 경로로

volumes:
  ollama_gpu0_data:



  # GPU 1 - gpt-oss:20b 전용 (건드리지 않음)
  ollama_gpu1:
    image: ollama/ollama
    container_name: ollama_gpu1
    ports:
      - "11435:11434"
    volumes:
      - ollama_gpu1_data:/root/.ollama
    restart: always
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=1
      - OLLAMA_KEEP_ALIVE=-1
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_NUM_PARALLEL=4

  # FastAPI Gateway (with optional GPU access, GPU 0)
  fastapi_app:
    build: ./fastapi_app
    container_name: fastapi_gateway
    ports:
      - "8010:8000"  # 외부에 8010으로 FastAPI 노출 (docs용)
    volumes:
      - api_db_data:/app/database
    restart: always
    depends_on:
      - ollama_gpu0
      - ollama_gpu1
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=0  # optional if FastAPI doesn't need GPU

  # NGINX Reverse Proxy
  nginx:
    image: nginx:latest
    container_name: nginx_proxy
    ports:
      - "80:80"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/conf.d/default.conf
    restart: always

volumes:
  ollama_gpu0_data:
  ollama_gpu1_data:
  api_db_data:
