# ğŸš€ AI ì„œë²„ ìµœì í™” ê°€ì´ë“œ

## ğŸ“Š ë¬¸ì œ ì§„ë‹¨ ìš”ì•½

### ğŸ”´ ë°œê²¬ëœ ë¬¸ì œë“¤
1. **Cold Start í˜„ìƒ**: ëª¨ë¸ì´ 4ë¶„ë§ˆë‹¤ ì–¸ë¡œë“œë˜ì–´ ë§¤ ìš”ì²­ì‹œ ì¬ë¡œë”© (16ì´ˆ+)
2. **íƒ€ì„ì•„ì›ƒ ì„¤ì •**: 120ì´ˆë¡œ ì œí•œë˜ì–´ ìˆì–´ OCRì´ ì™„ë£Œ ì „ 500 ì—ëŸ¬ ë°œìƒ
3. **ëª¨ë¸ ìŠ¤ì™‘**: ì—¬ëŸ¬ ëª¨ë¸ í…ŒìŠ¤íŠ¸ êµ¬ì¡°ë¡œ ì¸í•œ ë¶ˆí•„ìš”í•œ ë¡œë”©/ì–¸ë¡œë”©
4. **GPU ë¯¸í™œìš©**: RTX 5060 Tiê°€ ìœ íœ´ ìƒíƒœ

### ğŸ“ˆ ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ
| í•­ëª© | ê¸°ì¡´ | ìµœì í™” í›„ |
|------|------|-----------|
| ì²« ìš”ì²­ (Cold Start) | 16ì´ˆ + OCR 2ë¶„+ | 1ì´ˆ + OCR 3-5ì´ˆ |
| ì´í›„ ìš”ì²­ (Warm) | 6ì´ˆ + OCR ?ì´ˆ | **1ì´ˆ + OCR 2-3ì´ˆ** |
| ëª¨ë¸ ë¡œë”© | ë§¤ 4ë¶„ë§ˆë‹¤ | **ì„œë²„ ì‹œì‘ì‹œ 1íšŒ** |
| ì „ì²´ ì²˜ë¦¬ ì‹œê°„ | ~136ì´ˆ (2ë¶„+) | **~5ì´ˆ** âœ¨ |

**â†’ ì•½ 20-30ë°° ì„±ëŠ¥ í–¥ìƒ ì˜ˆìƒ!**

---

## ğŸ”§ ì ìš© ë°©ë²•

### 1ë‹¨ê³„: ë°±ì—…
```bash
cd /home/user/ollama-ai-server

# ê¸°ì¡´ íŒŒì¼ ë°±ì—…
cp docker-compose.production.yml docker-compose.production.backup.yml
cp fastapi_app/app/main.py fastapi_app/app/main.backup.py
cp fastapi_app/app/config.py fastapi_app/app/config.backup.py
```

### 2ë‹¨ê³„: ìµœì í™” íŒŒì¼ ì ìš©

**ë¡œì»¬ PC (Windows)ì—ì„œ ì„œë²„ë¡œ ì—…ë¡œë“œ:**

```powershell
# PowerShellì—ì„œ
cd C:\Users\User\Desktop\Yeji\ai_server

# SCPë¡œ ì„œë²„ì— ì—…ë¡œë“œ (ì˜ˆì‹œ)
scp docker-compose.production.optimized.yml user@server:/home/user/ollama-ai-server/docker-compose.production.yml
scp fastapi_app/app/main.optimized.py user@server:/home/user/ollama-ai-server/fastapi_app/app/main.py
scp fastapi_app/app/config.optimized.py user@server:/home/user/ollama-ai-server/fastapi_app/app/config.py
```

ë˜ëŠ” **ì„œë²„ì—ì„œ ì§ì ‘ ìˆ˜ì •:**

```bash
# ì„œë²„ SSH ì ‘ì† í›„
cd /home/user/ollama-ai-server

# config.py ìˆ˜ì •
nano fastapi_app/app/config.py
```

**config.pyë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ë³€ê²½:**
```python
OLLAMA_ENDPOINTS = {
    "qwen2.5vl:7b": "http://ollama_gpu0:11434",
    "gpt-oss:20b": "http://ollama_gpu1:11434"
}

SUPPORTED_MODELS = set(OLLAMA_ENDPOINTS.keys())
DATABASE_FILE = "/app/database/api_server.db"
OLLAMA_BASE_URL = "http://ollama_gpu0:11434"
OLLAMA_REQUEST_TIMEOUT = 300.0  # ğŸ”¥ ì¶”ê°€!
```

**docker-compose.production.yml í™˜ê²½ë³€ìˆ˜ ì¶”ê°€:**
```yaml
ollama_gpu0:
  environment:
    - NVIDIA_VISIBLE_DEVICES=0
    - OLLAMA_KEEP_ALIVE=-1              # ğŸ”¥ ì¶”ê°€!
    - OLLAMA_MAX_LOADED_MODELS=1        # ğŸ”¥ ì¶”ê°€!
    - OLLAMA_NUM_PARALLEL=4             # ğŸ”¥ ì¶”ê°€!

ollama_gpu1:
  environment:
    - NVIDIA_VISIBLE_DEVICES=1
    - OLLAMA_KEEP_ALIVE=-1              # ğŸ”¥ ì¶”ê°€!
    - OLLAMA_MAX_LOADED_MODELS=1        # ğŸ”¥ ì¶”ê°€!
    - OLLAMA_NUM_PARALLEL=4             # ğŸ”¥ ì¶”ê°€!
```

### 3ë‹¨ê³„: main.pyì— ì›Œë°ì—… ì½”ë“œ ì¶”ê°€

**fastapi_app/app/main.pyì˜ `@app.on_event("startup")` ë¶€ë¶„ ìˆ˜ì •:**

```python
@app.on_event("startup")
async def on_startup():
    database.init_db()
    
    # ğŸ”¥ ëª¨ë¸ ì›Œë°ì—… ì¶”ê°€
    print("ğŸš€ ëª¨ë¸ ì›Œë°ì—… ì‹œì‘...")
    
    async with httpx.AsyncClient(timeout=60.0) as client:
        try:
            # GPU 0: Qwen2.5-VL
            print("  â†³ GPU 0: qwen2.5vl:7b ë¡œë”©ì¤‘...")
            await client.post(
                "http://ollama_gpu0:11434/api/generate",
                json={"model": "qwen2.5vl:7b", "prompt": "warmup", "keep_alive": -1}
            )
            print("  âœ… GPU 0 ë¡œë“œ ì™„ë£Œ")
            
            # GPU 1: GPT-OSS
            print("  â†³ GPU 1: gpt-oss:20b ë¡œë”©ì¤‘...")
            await client.post(
                "http://ollama_gpu1:11434/api/generate",
                json={"model": "gpt-oss:20b", "prompt": "warmup", "keep_alive": -1}
            )
            print("  âœ… GPU 1 ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âš ï¸ ì›Œë°ì—… ì—ëŸ¬: {e}")
```

ê·¸ë¦¬ê³  ëª¨ë“  `ollama_payload`ì— `"keep_alive": -1` ì¶”ê°€:
```python
ollama_payload = {
    "model": model_name,
    "prompt": request.prompt,
    "keep_alive": -1,  # ğŸ”¥ ì¶”ê°€!
    # ...
}
```

### 4ë‹¨ê³„: ì¬ì‹œì‘

```bash
cd /home/user/ollama-ai-server

# ì»¨í…Œì´ë„ˆ ì¤‘ì§€ ë° ì œê±°
docker-compose -f docker-compose.production.yml down

# ì¬ë¹Œë“œ (ì½”ë“œ ë³€ê²½ì‚¬í•­ ë°˜ì˜)
docker-compose -f docker-compose.production.yml build fastapi_app

# ì‹œì‘
docker-compose -f docker-compose.production.yml up -d

# ë¡œê·¸ í™•ì¸ (ì›Œë°ì—… ê³¼ì • í™•ì¸)
docker logs -f fastapi_gateway
```

---

## âœ… ê²€ì¦ ë°©ë²•

### 1. ëª¨ë¸ì´ ë©”ëª¨ë¦¬ì— ìœ ì§€ë˜ëŠ”ì§€ í™•ì¸
```bash
# ì»¨í…Œì´ë„ˆ ì ‘ì†
docker exec -it ollama_gpu0 bash

# ëª¨ë¸ ìƒíƒœ í™•ì¸
ollama ps

# ë‹¤ìŒê³¼ ê°™ì´ ë³´ì—¬ì•¼ í•¨:
# NAME            UNTIL
# qwen2.5vl:7b    Forever  â† "Forever"ê°€ ì¤‘ìš”!
```

### 2. GPU ë©”ëª¨ë¦¬ ì‚¬ìš© í™•ì¸
```bash
# í˜¸ìŠ¤íŠ¸ì—ì„œ
nvidia-smi

# GPU 0: 7.5GB ì‚¬ìš© (qwen2.5vl:7b ë¡œë“œë¨)
# GPU 1: ~10GB ì‚¬ìš© (gpt-oss:20b ë¡œë“œë¨)
```

### 3. OCR ì†ë„ í…ŒìŠ¤íŠ¸
```bash
# í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ë¡œ OCR ìš”ì²­
time curl -X POST http://localhost:8010/v1/qwen/ocr \
  -H "X-API-Key: your-key" \
  -H "Content-Type: application/json" \
  -d '{
    "image_base64": "base64_encoded_image",
    "prompt": "í…ìŠ¤íŠ¸ ì¶”ì¶œ"
  }'

# ê²°ê³¼: processing_time_msê°€ 2000-5000ms (2-5ì´ˆ) ì •ë„ë©´ ì„±ê³µ!
```

### 4. í—¬ìŠ¤ ì²´í¬
```bash
curl http://localhost:8010/v1/health

# ì‘ë‹µ:
# {
#   "status": "healthy",
#   "gpu0_qwen": "online",
#   "gpu1_gpt": "online"
# }
```

---

## ğŸ¯ ì£¼ìš” ë³€ê²½ì‚¬í•­

### Docker Compose
- âœ… `OLLAMA_KEEP_ALIVE=-1` ì¶”ê°€ (ëª¨ë¸ ì˜êµ¬ ìœ ì§€)
- âœ… `OLLAMA_MAX_LOADED_MODELS=1` (GPUë‹¹ 1ê°œ ëª¨ë¸ë§Œ)
- âœ… `OLLAMA_NUM_PARALLEL=4` (ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™”)

### FastAPI Config
- âœ… `OLLAMA_REQUEST_TIMEOUT=300.0` (íƒ€ì„ì•„ì›ƒ 120ì´ˆâ†’300ì´ˆ)
- âœ… ëª¨ë¸ ëª©ë¡ ë‹¨ìˆœí™” (2ê°œ ê³ ì •)

### FastAPI Main
- âœ… ì„œë²„ ì‹œì‘ì‹œ ëª¨ë¸ ì›Œë°ì—… ì¶”ê°€
- âœ… ëª¨ë“  ìš”ì²­ì— `keep_alive=-1` ì¶”ê°€
- âœ… íƒ€ì„ì•„ì›ƒ ì„¤ì • ì ìš©

---

## ğŸ” íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë¬¸ì œ: ì›Œë°ì—… ì¤‘ íƒ€ì„ì•„ì›ƒ
```bash
# ì›Œë°ì—… íƒ€ì„ì•„ì›ƒì„ ëŠ˜ë¦¬ê¸°
# main.pyì˜ httpx.AsyncClient(timeout=60.0) â†’ timeout=120.0
```

### ë¬¸ì œ: GPU ë©”ëª¨ë¦¬ ë¶€ì¡±
```bash
# ì´ë¯¸ ë¡œë“œëœ ëª¨ë¸ ì œê±°
docker exec -it ollama_gpu0 bash
ollama stop qwen2.5vl:7b

# ì¬ì‹œì‘
docker-compose restart ollama_gpu0
```

### ë¬¸ì œ: ì—¬ì „íˆ ëŠë¦¼
```bash
# 1. Ollama ë¡œê·¸ í™•ì¸
docker logs ollama_gpu0 --tail 100

# 2. FastAPI ë¡œê·¸ í™•ì¸
docker logs fastapi_gateway --tail 100

# 3. nvidia-smië¡œ GPU ì‚¬ìš©ë¥  í™•ì¸
watch -n 1 nvidia-smi
```

---

## ğŸ“ ì¶”ê°€ ìµœì í™” íŒ

### 1. ì´ë¯¸ì§€ ì „ì²˜ë¦¬
OCR ì „ì— ì´ë¯¸ì§€ë¥¼ ë¦¬ì‚¬ì´ì§•í•˜ë©´ ë” ë¹ ë¦„:
```python
from PIL import Image
img = Image.open("input.jpg")
img.thumbnail((1920, 1920))  # ìµœëŒ€ 1920px
```

### 2. Batch Processing
ì—¬ëŸ¬ ì´ë¯¸ì§€ ë™ì‹œ ì²˜ë¦¬ì‹œ:
```python
# ìˆœì°¨ ì²˜ë¦¬ ëŒ€ì‹ 
async with asyncio.TaskGroup() as tg:
    tasks = [tg.create_task(ocr(img)) for img in images]
```

### 3. ëª¨ë‹ˆí„°ë§ ì¶”ê°€
```bash
# Prometheus + Grafana ì„¤ì •
# GPU ì‚¬ìš©ë¥ , ì²˜ë¦¬ ì‹œê°„, í ê¸¸ì´ ëª¨ë‹ˆí„°ë§
```

---

## ğŸ‰ ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] ë°±ì—… ì™„ë£Œ
- [ ] config.py ìˆ˜ì •
- [ ] docker-compose.yml í™˜ê²½ë³€ìˆ˜ ì¶”ê°€
- [ ] main.py ì›Œë°ì—… ì½”ë“œ ì¶”ê°€
- [ ] ì¬ë¹Œë“œ ë° ì¬ì‹œì‘
- [ ] `ollama ps`ë¡œ "Forever" í™•ì¸
- [ ] OCR ì†ë„ í…ŒìŠ¤íŠ¸ (5ì´ˆ ì´ë‚´)
- [ ] Health check í†µê³¼

**ëª¨ë“  ì²´í¬ë¦¬ìŠ¤íŠ¸ ì™„ë£Œì‹œ OCR ì²˜ë¦¬ ì‹œê°„ì´ 2ë¶„+ â†’ 3-5ì´ˆë¡œ ê°œì„ ë©ë‹ˆë‹¤!** ğŸš€
