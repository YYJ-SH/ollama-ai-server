version: '3.8'

services:
  # GPU 0용 Ollama - llama3, qwen, exaone
  ollama_gpu0:
    image: ollama/ollama
    container_name: ollama_gpu0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]
    ports:
      - "11436:11434"  # 외부 포트 11436으로 변경 (11434 충돌 회피)
    volumes:
      - ollama_gpu0_data:/root/.ollama
    restart: always

  # GPU 1용 Ollama - gpt-oss:20b
  ollama_gpu1:
    image: ollama/ollama
    container_name: ollama_gpu1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['1']
              capabilities: [gpu]
    ports:
      - "11435:11434"
    volumes:
      - ollama_gpu1_data:/root/.ollama
    restart: always

  # FastAPI 서버
  fastapi_app:
    build: ./fastapi_app
    container_name: fastapi_gateway
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]
    environment:
      - CUDA_VISIBLE_DEVICES=0
    volumes:
      - api_db_data:/app/database
    depends_on:
      - ollama_gpu0
      - ollama_gpu1
    restart: always

  # Nginx 리버스 프록시
  nginx:
    image: nginx:latest
    container_name: nginx_proxy
    ports:
      - "80:80"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/conf.d/default.conf
    depends_on:
      - fastapi_app
    restart: always

volumes:
  ollama_gpu0_data:
  ollama_gpu1_data:
  api_db_data:
