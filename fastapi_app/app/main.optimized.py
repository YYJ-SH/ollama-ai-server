import httpx
import base64
from fastapi import FastAPI, HTTPException, Header, Depends, UploadFile, File
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from typing import Optional, List
from . import config, database, models

app = FastAPI(
    title="Optimized AI API Server (Qwen2.5-VL + GPT-OSS)",
    description="ê³ ì„±ëŠ¥ 2-GPU ì „ìš© ì„œë²„: Qwen2.5-VL (OCR) + GPT-OSS (ë¶„ì„)"
)

# ğŸ”¥ ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ë¯¸ë¦¬ ë¡œë“œ (Warm-up)
@app.on_event("startup")
async def on_startup():
    database.init_db()
    
    # ğŸ”¥ ëª¨ë¸ ì›Œë°ì—…: ì„œë²„ ì‹œì‘ ì‹œ ë‘ ëª¨ë¸ ëª¨ë‘ ë¯¸ë¦¬ ë¡œë“œ
    print("ğŸš€ ëª¨ë¸ ì›Œë°ì—… ì‹œì‘...")
    
    async with httpx.AsyncClient(timeout=60.0) as client:
        try:
            # GPU 0: Qwen2.5-VL ì›Œë°ì—…
            print("  â†³ GPU 0: qwen2.5vl:7b ë¡œë”©ì¤‘...")
            await client.post(
                "http://ollama_gpu0:11434/api/generate",
                json={
                    "model": "qwen2.5vl:7b",
                    "prompt": "warmup",
                    "stream": False,
                    "keep_alive": -1  # ğŸ”¥ ì˜êµ¬ ìœ ì§€
                }
            )
            print("  âœ… GPU 0: qwen2.5vl:7b ë¡œë“œ ì™„ë£Œ")
            
            # GPU 1: GPT-OSS ì›Œë°ì—…
            print("  â†³ GPU 1: gpt-oss:20b ë¡œë”©ì¤‘...")
            await client.post(
                "http://ollama_gpu1:11434/api/generate",
                json={
                    "model": "gpt-oss:20b",
                    "prompt": "warmup",
                    "stream": False,
                    "keep_alive": -1  # ğŸ”¥ ì˜êµ¬ ìœ ì§€
                }
            )
            print("  âœ… GPU 1: gpt-oss:20b ë¡œë“œ ì™„ë£Œ")
            print("ğŸ‰ ëª¨ë“  ëª¨ë¸ ì›Œë°ì—… ì™„ë£Œ!")
            
        except Exception as e:
            print(f"âš ï¸ ì›Œë°ì—… ì¤‘ ì—ëŸ¬ (ë¬´ì‹œë¨): {e}")

# API í‚¤ ê²€ì¦ì„ ìœ„í•œ ì˜ì¡´ì„± ì£¼ì…
async def get_valid_api_key(x_api_key: str = Header(..., description="Your personal API Key.")):
    if not x_api_key:
        raise HTTPException(status_code=401, detail="API Key is missing")
    key_info = await database.validate_and_log_key(x_api_key)
    if not key_info:
        raise HTTPException(status_code=401, detail="Invalid or Inactive API Key")
    print(f"Request from '{key_info['owner']}' (Key: ...{x_api_key[-4:]})")
    return key_info

# [ê¸°ì¡´] ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ API
@app.get("/v1/models", tags=["Models"])
async def list_available_models(api_key: dict = Depends(get_valid_api_key)):
    """
    ê³ ì •ëœ 2ê°œ ëª¨ë¸ ë°˜í™˜ (ëª¨ë¸ ìŠ¤ì™‘ ì—†ìŒ)
    """
    return {
        "models": [
            {
                "name": "qwen2.5vl:7b",
                "gpu": "GPU 0 (RTX 3060)",
                "purpose": "OCR, ì´ë¯¸ì§€ ë¶„ì„",
                "size": "7B",
                "endpoint": config.OLLAMA_ENDPOINTS["qwen2.5vl:7b"]
            },
            {
                "name": "gpt-oss:20b",
                "gpu": "GPU 1 (RTX 5060 Ti)",
                "purpose": "ìƒì„¸ ë¶„ì„, ì¶”ë¡ ",
                "size": "20B",
                "endpoint": config.OLLAMA_ENDPOINTS["gpt-oss:20b"]
            }
        ]
    }

# [ê¸°ì¡´] ë©”ì¸ ìƒì„± API (íƒ€ì„ì•„ì›ƒ ì¦ê°€)
@app.post("/v1/generate", tags=["Generation"])
async def generate_completion(
    request: models.OllamaRequest,
    api_key: dict = Depends(get_valid_api_key)
):
    model_name = request.model.strip().lower()

    if model_name not in config.SUPPORTED_MODELS:
        raise HTTPException(
            status_code=400, 
            detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ì…ë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥: {list(config.SUPPORTED_MODELS)}"
        )

    endpoint = config.OLLAMA_ENDPOINTS.get(model_name)
    if not endpoint:
        raise HTTPException(status_code=500, detail=f"ëª¨ë¸ '{model_name}'ì— ëŒ€í•œ ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    ollama_payload = {
        "model": model_name,
        "prompt": request.prompt,
        "stream": request.stream,
        "options": request.options or {},
        "keep_alive": -1  # ğŸ”¥ ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì— ìœ ì§€
    }

    async with httpx.AsyncClient(timeout=config.OLLAMA_REQUEST_TIMEOUT) as client:
        try:
            response = await client.post(
                f"{endpoint}/api/generate",
                json=ollama_payload
            )
            response.raise_for_status()
            response_data = response.json()

            # ë¡œê·¸ ì €ì¥
            try:
                ai_response_text = response_data.get("response", "")
                await database.add_api_log(
                    owner=api_key.get("owner", "unknown"),
                    model=model_name,
                    prompt=request.prompt,
                    response=ai_response_text
                )
            except Exception as log_e:
                print(f"ë¡œê·¸ ê¸°ë¡ ì¤‘ ì—ëŸ¬ ë°œìƒ: {log_e}")

            return response_data

        except httpx.TimeoutException:
            raise HTTPException(
                status_code=504, 
                detail=f"ìš”ì²­ íƒ€ì„ì•„ì›ƒ ({config.OLLAMA_REQUEST_TIMEOUT}ì´ˆ ì´ˆê³¼)"
            )
        except httpx.RequestError as e:
            raise HTTPException(status_code=500, detail=f"Ollama ì—°ê²° ì˜¤ë¥˜: {e}")

# ==================== ğŸ”¥ ìµœì í™”ëœ Qwen2.5-VL OCR ì—”ë“œí¬ì¸íŠ¸ ====================

class QwenOCRRequest(BaseModel):
    """Qwen2.5-VL OCR ìš”ì²­ ëª¨ë¸"""
    image_base64: str
    prompt: Optional[str] = "ì´ ì´ë¯¸ì§€ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì •í™•íˆ ì½ì–´ì£¼ì„¸ìš”. í•œêµ­ì–´, ì˜ì–´, ìˆ«ìë¥¼ ëª¨ë‘ í¬í•¨í•´ì„œ ì¤„ë°”ê¿ˆë„ ìœ ì§€í•´ì£¼ì„¸ìš”."
    temperature: Optional[float] = 0.1
    top_p: Optional[float] = 0.9

class QwenOCRResponse(BaseModel):
    """Qwen2.5-VL OCR ì‘ë‹µ ëª¨ë¸"""
    success: bool
    ocr_text: str
    model_used: str
    processing_time_ms: float
    error: Optional[str] = None

@app.post("/v1/qwen/ocr", tags=["Qwen2.5-VL"], response_model=QwenOCRResponse)
async def qwen_ocr_endpoint(
    request: QwenOCRRequest,
    api_key: dict = Depends(get_valid_api_key)
):
    """
    ğŸš€ ìµœì í™”ëœ Qwen2.5-VL OCR ì—”ë“œí¬ì¸íŠ¸
    
    - ëª¨ë¸ ê³ ì • (qwen2.5vl:7b only)
    - íƒ€ì„ì•„ì›ƒ 300ì´ˆ
    - ëª¨ë¸ ì˜êµ¬ ë©”ëª¨ë¦¬ ìœ ì§€
    """
    import time
    start_time = time.time()

    try:
        # ğŸ”¥ ëª¨ë¸ ê³ ì •
        model = "qwen2.5vl:7b"
        
        # Qwen2.5-VL ì „ìš© í˜ì´ë¡œë“œ êµ¬ì„±
        qwen_payload = {
            "model": model,
            "prompt": request.prompt,
            "images": [request.image_base64],
            "stream": False,
            "keep_alive": -1,  # ğŸ”¥ ëª¨ë¸ ìœ ì§€
            "options": {
                "temperature": request.temperature,
                "top_p": request.top_p
            }
        }

        endpoint = config.OLLAMA_ENDPOINTS[model]

        async with httpx.AsyncClient(timeout=config.OLLAMA_REQUEST_TIMEOUT) as client:
            try:
                response = await client.post(
                    f"{endpoint}/api/generate",
                    json=qwen_payload,
                    headers={'Content-Type': 'application/json'}
                )
                response.raise_for_status()
                result = response.json()

                processing_time = (time.time() - start_time) * 1000
                ocr_text = result.get("response", "").strip()

                if not ocr_text:
                    ocr_text = "[No text detected]"

                # DB ë¡œê·¸ ê¸°ë¡
                try:
                    await database.add_api_log(
                        owner=api_key.get("owner", "unknown"),
                        model=model,
                        prompt=f"[OCR] {request.prompt[:100]}...",
                        response=ocr_text[:500]
                    )
                except Exception as log_e:
                    print(f"OCR ë¡œê·¸ ê¸°ë¡ ì¤‘ ì—ëŸ¬: {log_e}")

                return QwenOCRResponse(
                    success=True,
                    ocr_text=ocr_text,
                    model_used=model,
                    processing_time_ms=round(processing_time, 2),
                    error=None
                )

            except httpx.TimeoutException:
                return QwenOCRResponse(
                    success=False,
                    ocr_text="",
                    model_used=model,
                    processing_time_ms=round((time.time() - start_time) * 1000, 2),
                    error=f"OCR processing timeout ({config.OLLAMA_REQUEST_TIMEOUT}s exceeded)"
                )
            except httpx.RequestError as e:
                return QwenOCRResponse(
                    success=False,
                    ocr_text="",
                    model_used=model,
                    processing_time_ms=round((time.time() - start_time) * 1000, 2),
                    error=f"Network error: {str(e)}"
                )

    except Exception as e:
        return QwenOCRResponse(
            success=False,
            ocr_text="",
            model_used="qwen2.5vl:7b",
            processing_time_ms=round((time.time() - start_time) * 1000, 2),
            error=f"Server error: {str(e)}"
        )

@app.post("/v1/qwen/ocr-file", tags=["Qwen2.5-VL"], response_model=QwenOCRResponse)
async def qwen_ocr_file_upload(
    file: UploadFile = File(..., description="ì´ë¯¸ì§€ íŒŒì¼ (PNG, JPG, JPEG)"),
    prompt: str = "ì´ ì´ë¯¸ì§€ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì •í™•íˆ ì½ì–´ì£¼ì„¸ìš”. í•œêµ­ì–´, ì˜ì–´, ìˆ«ìë¥¼ ëª¨ë‘ í¬í•¨í•´ì„œ ì¤„ë°”ê¿ˆë„ ìœ ì§€í•´ì£¼ì„¸ìš”.",
    temperature: float = 0.1,
    top_p: float = 0.9,
    api_key: dict = Depends(get_valid_api_key)
):
    """
    íŒŒì¼ ì—…ë¡œë“œ ë°©ì‹ì˜ Qwen2.5-VL OCR ì—”ë“œí¬ì¸íŠ¸
    """
    import time
    start_time = time.time()

    # íŒŒì¼ íƒ€ì… ê²€ì¦
    if not file.content_type.startswith('image/'):
        raise HTTPException(status_code=400, detail="Only image files are supported")

    try:
        # íŒŒì¼ì„ base64ë¡œ ì¸ì½”ë”©
        file_content = await file.read()
        image_base64 = base64.b64encode(file_content).decode('utf-8')

        # ê¸°ì¡´ OCR ì—”ë“œí¬ì¸íŠ¸ ì¬ì‚¬ìš©
        request_obj = QwenOCRRequest(
            image_base64=image_base64,
            prompt=prompt,
            temperature=temperature,
            top_p=top_p
        )

        return await qwen_ocr_endpoint(request_obj, api_key)

    except Exception as e:
        return QwenOCRResponse(
            success=False,
            ocr_text="",
            model_used="qwen2.5vl:7b",
            processing_time_ms=round((time.time() - start_time) * 1000, 2),
            error=f"File processing error: {str(e)}"
        )

@app.get("/v1/health", tags=["System"])
async def health_check():
    """
    ì „ì²´ ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ (ì¸ì¦ ë¶ˆí•„ìš”)
    """
    async with httpx.AsyncClient(timeout=5.0) as client:
        gpu0_status = "offline"
        gpu1_status = "offline"
        
        try:
            response = await client.get("http://ollama_gpu0:11434/api/tags")
            if response.status_code == 200:
                gpu0_status = "online"
        except:
            pass
            
        try:
            response = await client.get("http://ollama_gpu1:11434/api/tags")
            if response.status_code == 200:
                gpu1_status = "online"
        except:
            pass
    
    return {
        "status": "healthy" if gpu0_status == "online" and gpu1_status == "online" else "degraded",
        "gpu0_qwen": gpu0_status,
        "gpu1_gpt": gpu1_status,
        "models": {
            "qwen2.5vl:7b": "GPU 0 (RTX 3060)",
            "gpt-oss:20b": "GPU 1 (RTX 5060 Ti)"
        }
    }
